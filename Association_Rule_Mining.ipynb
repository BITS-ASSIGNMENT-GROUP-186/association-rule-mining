{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx \n",
    "\n",
    "from apyori import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.frequent_patterns import apriori #fpmax, fpgrowth\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_input_file = os.path.join(os.getcwd(), 'Employee_skills_traits.csv')\n",
    "employee_skills_df = pd.read_csv(path_to_input_file)\n",
    "employee_skills_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing whitespaces from column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "employee_skills_df.columns = employee_skills_df.columns.str.strip()\n",
    "employee_skills_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dropping duplicate records, if any\n",
    "##### Note: The dataset doesn't really have any duplicate records, it is just the ID which seems to be duplicated, all the other attributes are different even for the same id and I dont think we can just drop those records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"Dimension of the data before deleting duplicates - \",employee_skills_df.shape)\n",
    "\n",
    "# duplicate_rows= employee_skills_df[employee_skills_df.duplicated(['ID'],keep=False)]\n",
    "# print(\"Number of duplicate records - \", sum(employee_skills_df.duplicated(['ID'])))\n",
    "\n",
    "# if not duplicate_rows.empty:\n",
    "#     employee_skills_df.drop_duplicates(['ID'],keep='first',inplace=True)\n",
    "\n",
    "# print(\"Dimension of the data after deleting duplicates - \",employee_skills_df.shape)\n",
    "\n",
    "# The dataset doesn't really have any duplicate records, it is just the ID which seems to be duplicated, \n",
    "# all the other attributes are different even for the same id and I dont think we can just drop those records\n",
    "\n",
    "employee_skills_df.shape\n",
    "duplicate_records = employee_skills_df[employee_skills_df.duplicated(keep=False)]\n",
    "if not duplicate_records.empty:\n",
    "    employee_skills_df.drop_duplicates(keep=False, inplace=True)\n",
    "employee_skills_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Understanding correlation between data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation = employee_skills_df.corr()\n",
    "correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing columns which are not relevant to finding associations\n",
    "#### Studying the correlation table we see that ID and Gender are having negative or negligent correlations with most of the other attributes and can be considered irrelevant to finding out associations, it will be wise to drop them before applying the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "employee_skills_df.drop(columns=['ID', 'Gender'], inplace=True)\n",
    "employee_skills_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Conversion/Normalization\n",
    "#### Here we will convert and normalize numeric attributes such as Employment Period, Age and Time in current department to categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalizing employment period values through equal width binning\n",
    "\n",
    "employee_skills_df['Employment period'].describe()\n",
    "\n",
    "employment_period_bin_label = ['0-5', '6-10', '11-15', '16-20']\n",
    "cut_bins_employment_period = [0, 5, 10, 15, 20]\n",
    "employee_skills_df['Employment period'] = pd.cut(employee_skills_df['Employment period'], bins=cut_bins_employment_period, labels=employment_period_bin_label)\n",
    "employee_skills_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalizing age values through equal width binning\n",
    "\n",
    "employee_skills_df['Age'].describe()\n",
    "\n",
    "age_bin_label = ['20-30', '31-40', '41-50', '51-60']\n",
    "cut_bins_age = [20, 30, 40, 50, 60]\n",
    "employee_skills_df['Age'] = pd.cut(employee_skills_df['Age'], bins=cut_bins_age, labels=age_bin_label)\n",
    "employee_skills_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalizing employee's time in current department values through equal width binning\n",
    "\n",
    "employee_skills_df['Time in current department'].describe()\n",
    "\n",
    "current_department_bin_label = ['0-3', '4-6', '7-9', '10-12']\n",
    "cut_bins_curr_dept = [0, 3, 6, 9, 12]\n",
    "employee_skills_df['Time in current department'] = pd.cut(employee_skills_df['Time in current department'], bins=cut_bins_curr_dept, labels=current_department_bin_label)\n",
    "employee_skills_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Converting categorical variables to series of ones and zeros quantification and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "employee_skills_df = pd.get_dummies(employee_skills_df, columns=['Employment period', 'Age', 'Time in current department'])\n",
    "employee_skills_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding frequent itemsets from the dataset. We start with support value of 0.5 and stop when we have enough frequent itemsets to extract rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "support_list = [0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "\n",
    "for support in support_list:\n",
    "    frequent_itemsets = apriori(employee_skills_df, min_support=support, use_colnames=True)\n",
    "    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "    print(f\"Minimum support is {support*100}%\")\n",
    "    print(frequent_itemsets)\n",
    "    print(\"-----------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on the observations above, we choose the frequent itemsets by keeping the min support as 0.10 as we get enough itemsets to extract meaningful rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frequent_itemsets\n",
    "rules = association_rules(frequent_itemsets,  metric=\"lift\", min_threshold=1)\n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Total rules generated are {rules.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering rules with lift >=1 , confidence >= 0.55 and support >= 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rules = rules[(rules['confidence']>=0.55) & (rules['lift']>=1) & (rules['support']>=0.15)]\n",
    "# Sorting rules in descending order by confidence\n",
    "rules.sort_values(by='confidence',ascending=False,inplace=True)\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing rules in format of {antecedents} ---> {consequents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in rules.index:\n",
    "    print(f\"{list(rules['antecedents'][idx])} ====> {list(rules['consequents'][idx])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot to look at support and confidence values of selected rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "support=rules.as_matrix(columns=['support'])\n",
    "confidence=rules.as_matrix(columns=['confidence'])\n",
    "plt.scatter(support, confidence, alpha=0.5, marker=\"*\")\n",
    "plt.xlabel('support')\n",
    "plt.ylabel('confidence') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using directed graph to look at associations between the rules. \n",
    "\n",
    "- Yellow dots represent the rules\n",
    "- Green dots shows antecedents and consequents\n",
    "- Incoming arrows into a rule shows the antecendents\n",
    "- Outgoing arrows from a rule shows the consequents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rules_to_show = 8\n",
    "\n",
    "G1 = nx.DiGraph()\n",
    "\n",
    "color_map=[]\n",
    "N = 50\n",
    "colors = np.random.rand(N)    \n",
    "strs=['R0', 'R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7']   \n",
    "\n",
    "\n",
    "for i in range (rules_to_show):      \n",
    "    G1.add_nodes_from([\"R\"+str(i)])\n",
    "\n",
    "    for a in rules.iloc[i]['antecedents']:\n",
    "        G1.add_nodes_from([a])\n",
    "        G1.add_edge(a, \"R\"+str(i), color=colors[i] , weight = 2)\n",
    "\n",
    "    for c in rules.iloc[i]['consequents']:\n",
    "        G1.add_nodes_from([c])\n",
    "        G1.add_edge(\"R\"+str(i), c, color=colors[i],  weight=2)\n",
    "\n",
    "for node in G1:\n",
    "   found_a_string = False\n",
    "   for item in strs: \n",
    "       if node==item:\n",
    "            found_a_string = True\n",
    "   if found_a_string:\n",
    "        color_map.append('yellow')\n",
    "   else:\n",
    "        color_map.append('green')       \n",
    "\n",
    "\n",
    "edges = G1.edges()\n",
    "colors = [G1[u][v]['color'] for u,v in edges]\n",
    "weights = [G1[u][v]['weight'] for u,v in edges]\n",
    "\n",
    "pos = nx.spring_layout(G1, k=30, scale=1)\n",
    "nx.draw(G1, pos, edges=edges, node_color = color_map, edge_color=colors, width=weights, font_size=16, with_labels=False)            \n",
    "\n",
    "for p in pos:  # raise text positions\n",
    "    pos[p][1] += 0.07\n",
    "nx.draw_networkx_labels(G1, pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
